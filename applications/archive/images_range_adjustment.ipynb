{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c694bcc",
   "metadata": {},
   "source": [
    "# Images range adjustment\n",
    "\n",
    "This notebook is designed to calculate the range adjustment that needs to be applied to CT and PET images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "\n",
    "import env_apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98de32",
   "metadata": {},
   "source": [
    "## Images extraction\n",
    "\n",
    "We first extract the prostate pixel values from all the images of the training patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e39b2",
   "metadata": {},
   "source": [
    "We define the `patients_data_extractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delia.databases import PatientsDatabase\n",
    "from delia.extractors import PatientsDataExtractor\n",
    "from delia.transforms import (\n",
    "    MatchingCentroidSpatialCropD,\n",
    "    MatchingCropForegroundD,\n",
    "    MatchingResampleD,\n",
    "    PETtoSUVD,\n",
    "    ResampleD\n",
    ")\n",
    "from monai.transforms import (\n",
    "    CenterSpatialCropD,\n",
    "    Compose,\n",
    "    KeepLargestConnectedComponentD,\n",
    "    ScaleIntensityD,\n",
    "    SpatialCropD,\n",
    "    ThresholdIntensityD,\n",
    ")\n",
    "\n",
    "\n",
    "env_apps.configure_logging(\"logging_conf.yaml\")\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        ResampleD(keys=[\"CT\"], out_spacing=(1.0, 1.0, 1.0)),\n",
    "        MatchingResampleD(reference_image_key=\"CT\", matching_keys=[\"PT\", \"Prostate\"]),\n",
    "        MatchingCropForegroundD(reference_image_key=\"CT\", matching_keys=[\"PT\", \"Prostate\"]),\n",
    "        SpatialCropD(keys=[\"CT\", \"PT\", \"Prostate\"], roi_slices=[slice(30, 740), slice(None), slice(None)]),\n",
    "        KeepLargestConnectedComponentD(keys=[\"Prostate\"]),\n",
    "        MatchingCentroidSpatialCropD(segmentation_key=\"Prostate\", matching_keys=[\"CT\", \"PT\"], roi_size=(128, 128, 128)),\n",
    "        PETtoSUVD(keys=[\"PT\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "patients_data_extractor = PatientsDataExtractor(\n",
    "    path_to_patients_folder=r\"local_data/Learning_set\",\n",
    "    series_descriptions=r\"local_data/series_descriptions.json\",\n",
    "    transforms=transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bcd2c",
   "metadata": {},
   "source": [
    "We extract the patients' data. This step takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967275a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CT_pixel_values_in_all_patients_roi = []\n",
    "PT_pixel_values_in_all_patients_roi = []\n",
    "for i, patient_data in enumerate(patients_data_extractor):\n",
    "    print(f\"{'-'*20}\\nPatient ID: {patient_data.patient_id}\")\n",
    "\n",
    "    for patient_image_data in patient_data.data:\n",
    "        dicom_header = patient_image_data.image.dicom_header\n",
    "        numpy_array_image = patient_image_data.image.numpy_array\n",
    "        \n",
    "        if dicom_header.Modality == \"CT\":\n",
    "            ct_array = numpy_array_image\n",
    "        if dicom_header.Modality == \"PT\":\n",
    "            pt_array = numpy_array_image\n",
    "\n",
    "        segmentations = patient_image_data.segmentations\n",
    "        if segmentations:\n",
    "            for organ, numpy_array_image in segmentations[0].numpy_array_label_maps.items():\n",
    "                if organ == \"Prostate\":\n",
    "                    prostate_mask_array = numpy_array_image\n",
    "                    \n",
    "    ct_roi = ct_array[np.where(prostate_mask_array)]\n",
    "    pt_roi = pt_array[np.where(prostate_mask_array)]\n",
    "    \n",
    "    CT_pixel_values_in_all_patients_roi.extend(ct_roi)\n",
    "    PT_pixel_values_in_all_patients_roi.extend(pt_roi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4e834",
   "metadata": {},
   "source": [
    "We copy the data to avoid overwriting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_pixels = deepcopy(CT_pixel_values_in_all_patients_roi)\n",
    "pt_pixels = deepcopy(PT_pixel_values_in_all_patients_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057aeb5",
   "metadata": {},
   "source": [
    "## CT range adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569466c",
   "metadata": {},
   "source": [
    "We follow the methodology described in:\n",
    "\n",
    "> Shahedi M, Halicek M, Guo R, Zhang G, Schuster DM, Fei B. A semiautomatic segmentation method for prostate in CT images using local texture classification and statistical shape modeling. Med Phys. 2018 Jun;45(6):2527-2541. doi: 10.1002/mp.12898. Epub 2018 Apr 23. PMID: 29611216; PMCID: PMC6149529.\n",
    "\n",
    "The voxels with HU values below -200 and above 250 were assigned HU values of -200 and 250, respectively. We determined these HU threshold levels by observing the HU range for prostate voxels across the training images and removing the 0.2% outliers. The resulting HU values ranged from -178 to 244, so we arbitrarly selected the -200 to 250 range as a slightly larger range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42cde81",
   "metadata": {},
   "source": [
    "First, we calculate the HU range after removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43b27999",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of outliers removed: 0.19983215275543031 %\n",
      "Range: [-178, 244] HU\n",
      "Mean: 32.52880449817627 HU\n",
      "Median: 33.0 HU\n"
     ]
    }
   ],
   "source": [
    "ct_pixels = np.array(ct_pixels)\n",
    "\n",
    "def reject_outliers(data, m=5.5869):\n",
    "    filtering = np.absolute(data - np.mean(data)) < m * np.std(data)\n",
    "    return data[filtering]\n",
    "\n",
    "cleaned_ct_pixels = reject_outliers(ct_pixels)\n",
    "\n",
    "print(f\"Percentage of outliers removed: {(1 - len(cleaned_ct_pixels)/len(ct_pixels))*100} %\")\n",
    "print(f\"Range: [{np.min(cleaned_ct_pixels)}, {np.max(cleaned_ct_pixels)}] HU\")\n",
    "print(f\"Mean: {np.mean(cleaned_ct_pixels)} HU\")\n",
    "print(f\"Median: {np.median(cleaned_ct_pixels)} HU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fdcc8",
   "metadata": {},
   "source": [
    "We then clip the pixels' values to our arbirary range (based on the range obtained with the previous step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3eb74f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of outliers removed: 0.18646426134334654 %\n",
      "Range: [-200, 250] HU\n",
      "Mean: 32.85776447682485 HU\n",
      "Median: 34.0 HU\n"
     ]
    }
   ],
   "source": [
    "clipped_ct_pixels = np.clip(ct_pixels, -200, 250)\n",
    "number_of_outliers = len(ct_pixels[(ct_pixels < -200) | (ct_pixels > 250)])\n",
    "\n",
    "print(f\"Percentage of outliers removed: {(number_of_outliers/len(ct_pixels))*100} %\")\n",
    "print(f\"Range: [{np.min(clipped_ct_pixels)}, {np.max(clipped_ct_pixels)}] HU\")\n",
    "print(f\"Mean: {np.mean(clipped_ct_pixels)} HU\")\n",
    "print(f\"Median: {np.median(clipped_ct_pixels)} HU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e471664",
   "metadata": {},
   "source": [
    "We compute the global CT images histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60257cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    ct_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(ct_pixels)) / len(ct_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    ct_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(ct_pixels)) / len(ct_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f38751",
   "metadata": {},
   "source": [
    "We compute the clipped CT images histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23edd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    clipped_ct_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(clipped_ct_pixels)) / len(clipped_ct_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    clipped_ct_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(clipped_ct_pixels)) / len(clipped_ct_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b754751",
   "metadata": {},
   "source": [
    "## PT range adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c2556",
   "metadata": {},
   "source": [
    "To ensure consistency and avoid potential inaccuracies, we applied a threshold of 25 to the Standardized Uptake Value (SUV) for all voxels with SUV values above this limit. This threshold was determined based on the maximum SUV value (24.9) observed in our training images, as determined by the radiologist who reviewed our imaging data. By capping the SUV values above 25, we are able to maintain a standardized approach that reduces the impact of any variability in tracer uptake, and ensures a more reliable and reproducible assessment of metabolic activity in our imaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41356e1",
   "metadata": {},
   "source": [
    "We clip the pixels' values to our arbirary range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06d63895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of outliers removed: 0.06613541477938033 %\n",
      "Range: [0.0, 25.0] SUV\n",
      "Mean: 2.4371612121191992 SUV\n",
      "Median: 2.1341411159528114 SUV\n"
     ]
    }
   ],
   "source": [
    "pt_pixels = np.array(pt_pixels)\n",
    "\n",
    "clipped_pt_pixels = np.clip(pt_pixels, 0, 25)\n",
    "number_of_outliers = len(pt_pixels[(pt_pixels < 0) | (pt_pixels > 25)])\n",
    "\n",
    "print(f\"Percentage of outliers removed: {(number_of_outliers/len(pt_pixels))*100} %\")\n",
    "print(f\"Range: [{np.min(clipped_pt_pixels)}, {np.max(clipped_pt_pixels)}] SUV\")\n",
    "print(f\"Mean: {np.mean(clipped_pt_pixels)} SUV\")\n",
    "print(f\"Median: {np.median(clipped_pt_pixels)} SUV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17cded",
   "metadata": {},
   "source": [
    "We compute the global PT images histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91619ec2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    pt_pixels,\n",
    "    bins=25,\n",
    "    weights=np.ones(len(pt_pixels)) / len(pt_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    pt_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(pt_pixels)) / len(pt_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d4456",
   "metadata": {},
   "source": [
    "We compute the clipped PT images histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26149395",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pt_pixels = np.clip(pt_pixels, 0, 25)\n",
    "\n",
    "plt.hist(\n",
    "    cleaned_pt_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(cleaned_pt_pixels)) / len(cleaned_pt_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    cleaned_pt_pixels, \n",
    "    bins=25, \n",
    "    weights=np.ones(len(cleaned_pt_pixels)) / len(cleaned_pt_pixels)\n",
    ")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
